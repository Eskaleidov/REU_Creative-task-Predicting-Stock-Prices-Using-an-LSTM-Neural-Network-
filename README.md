# REU_Creative-task-Predicting-Stock-Prices-Using-an-LSTM-Neural-Network-
Министерство науки и высшего образования Российской Федерации
Федеральное государственное бюджетное образовательное учреждение
высшего образования
«Российский экономический университет имени Г.В. Плеханова»
Высшая школа кибертехнологий математики и статистики

Кафедра: Прикладной информатики и информационной безопасности
Направление: Прикладная информатика
Уровень: Бакалавр
Дисциплина: Структуры данных и алгоритмы


Творческое задание:
“Прогнозирование стоимости акций с использованием нейронной сети LSTM”




Выполнили:
студенты 1 курса группы 15.27Д-ПИ03/25б
Зиновьев Максим Александрович, Соломатин Иван Петрович








Москва, 2025 г.


Прогнозирование стоимости акций с использованием нейронной сети LSTM
________________________________________
Введение
Развитие финансовых рынков и компьютерных технологий создало потребность в точном прогнозировании цен акций. Традиционные экономические модели и статистические методы часто не могут адекватно описать сложные нелинейные зависимости, присутствующие на финансовых рынках. Недавние успехи в области глубокого обучения открыли новые возможности для анализа временных рядов, в том числе для предсказания динамики цен финансовых инструментов.
Нейронные сети, особенно рекуррентные архитектуры, продемонстрировали свою эффективность в работе с последовательными данными, выявляя скрытые закономерности и долгосрочные зависимости. В данной работе реализована система прогнозирования котировок акций на основе архитектуры LSTM (Long Short-Term Memory) с использованием Python и фреймворка TensorFlow/Keras.
Цель работы
Разработать и реализовать модель глубокого обучения на базе LSTM для прогнозирования цен закрытия акций с высокой точностью и предоставить интерактивное веб-приложение для визуализации результатов.
Задачи исследования
1.	Анализ существующих методов прогнозирования временных рядов
2.	Изучение и реализация архитектуры LSTM
3.	Обработка и подготовка реальных исторических данных котировок
4.	Оптимизация гиперпараметров модели
5.	Оценка качества прогнозов с использованием метрик MAPE, MAE, R²
6.	Разработка веб-интерфейса для интерактивного анализа
________________________________________
Глава 1: Описание аналогов и существующие подходы
1.1 ARIMA (AutoRegressive Integrated Moving Average)
ARIMA — это классическая эконометрическая модель, широко используемая для прогнозирования временных рядов. Она основана на трёх компонентах:
•	AR (AutoRegressive): регрессионная зависимость от предыдущих значений
•	I (Integrated): дифференцирование для достижения стационарности
•	MA (Moving Average): использование ошибок прогноза прошлых периодов
Преимущества:
•	Простота интерпретации
•	Хорошо работает со стационарными данными
•	Наличие теоретической базы
Недостатки:
•	Предполагает линейные зависимости
•	Плохо справляется с нелинейными и сложными паттернами
•	Требует преобразования нестационарных рядов
Эмпирические результаты: По результатам исследований, ARIMA показывает среднюю абсолютную процентную ошибку (MAPE) на уровне 15-20% при прогнозировании цен акций, в то время как LSTM достигает значений 2-8%.
1.2 SARIMA (Seasonal ARIMA)
SARIMA — расширение ARIMA, учитывающее сезонные колебания во временных рядах. Добавляет дополнительные параметры для моделирования сезонности.
Особенности:
•	Эффективна для данных с явной периодичностью
•	Более гибкая, чем ARIMA
•	Требует правильной идентификации сезонного периода
Ограничения:
•	Также предполагает линейность
•	Сложность подбора параметров
•	Менее адаптивна к нелинейным изменениям рынка
1.3 GRU (Gated Recurrent Unit)
GRU — упрощённая версия LSTM, разработанная для преодоления проблемы исчезающего градиента в традиционных рекуррентных нейронных сетях.
Структура GRU:
•	Update Gate (ворота обновления): определяет, сколько информации из прошлого нужно сохранить
•	Reset Gate (ворота сброса): контролирует, насколько забывчива сеть
Сравнение с LSTM:
•	Меньше параметров (более вычислительно эффективна)
•	Быстрее обучается
•	Иногда показывает сравнимые результаты на небольших датасетах
•	Менее адаптивна к очень длинным зависимостям
1.4 CNN-LSTM (Гибридная архитектура)
CNN-LSTM объединяет свёрточные нейронные сети (CNN) для извлечения локальных признаков с LSTM для моделирования долгосрочных зависимостей.
Архитектура:
•	CNN слои: обнаруживают локальные паттерны и коротко-временные тренды
•	LSTM слои: захватывают долгосрочные и последовательные зависимости
Преимущества:
•	Лучшее снижение шума в данных
•	Более точное выявление многомасштабных паттернов
•	Улучшенная обработка волатильности рынка
Результаты: Эмпирические исследования показывают, что CNN-LSTM обеспечивает на 5-15% лучшие результаты по сравнению с чистой LSTM на волатильных финансовых данных.
1.5 Transformer и Attention Mechanism
Transformer архитектура, представленная в работе "Attention is All You Need" (2017), использует механизм Self-Attention вместо рекуррентных слоёв.
Ключевые компоненты:
•	Query, Key, Value матрицы (Q, K, V): позволяют модели сосредоточиться на релевантных частях последовательности
•	Multi-Head Attention: несколько "голов" внимания обрабатывают разные аспекты данных
•	Параллелизируемость: в отличие от LSTM, позволяет обрабатывать всю последовательность параллельно
Преимущества:
•	Лучше захватывает долгие зависимости
•	Значительно быстрее обучается
•	Меньше проблем с исчезающим градиентом
Недостатки:
•	Требует больше вычислительных ресурсов
•	Более сложна в интерпретации
•	Требует больших объёмов данных для оптимального обучения
Сравнительная таблица методов
Метод	Точность	Сложность	Интерпретируемость	Скорость обучения	Применимость
ARIMA	Средняя (15-20%)	Низкая	Высокая	Высокая	Стационарные ряды
SARIMA	Средняя (12-18%)	Средняя	Средняя	Средняя	Ряды с сезонностью
GRU	Высокая (3-8%)	Средняя	Низкая	Средняя	Малые датасеты
LSTM	Высокая (2-8%)	Средняя	Низкая	Средняя	Универсальный выбор
CNN-LSTM	Очень высокая (2-6%)	Высокая	Низкая	Низкая	Волатильные ряды
Transformer	Очень высокая (1-5%)	Высокая	Низкая	Очень низкая	Большие датасеты
Вывод: Для прогнозирования цен акций LSTM представляет оптимальный баланс между точностью, вычислительной эффективностью и простотой реализации. Архитектура хорошо зарекомендовала себя в промышленных приложениях.
________________________________________
Глава 2: Описание архитектуры и принципов работы LSTM
2.1 Проблема исчезающего градиента в RNN
Традиционные рекуррентные нейронные сети (RNN) имеют принципиальное ограничение: при обратном распространении ошибки через временные шаги градиент экспоненциально уменьшается, что делает невозможным обучение долгосрочным зависимостям. Это явление называется проблемой исчезающего градиента (vanishing gradient problem).
При обучении на длинных последовательностях градиент становится настолько малым, что практически не влияет на обновление весов нейронной сети на ранних временных шагах. Это препятствует модели в обучении долгосрочным паттернам, которые могут быть критичны для прогнозирования финансовых данных.
2.2 Структура LSTM ячейки
LSTM была разработана Hochreiter и Schmidhuber (1997) для решения этой проблемы. Её отличительная особенность — наличие состояния ячейки (cell state), которое проходит через всю последовательность с минимальными изменениями.
Основные компоненты LSTM ячейки:
2.2.1 Ворота забывания (Forget Gate)
Определяет, какую часть информации из предыдущего состояния ячейки нужно забыть. Выход находится в диапазоне от 0 до 1:
•	Значение близкое к 0 означает полностью забыть информацию
•	Значение близкое к 1 означает полностью сохранить информацию
Ворота забывания получают на вход предыдущее скрытое состояние и текущий входной сигнал, затем пропускают результат через сигмоидальную функцию активации.
2.2.2 Ворота ввода (Input Gate)
Контролирует, какую часть новой информации добавить в состояние ячейки. Также выдаёт значение в диапазоне от 0 до 1. Высокое значение означает, что новая информация важна и должна быть добавлена в состояние ячейки.
2.2.3 Кандидат ячейки (Cell Candidate)
Представляет новые возможные значения, которые могут быть добавлены в состояние ячейки. Использует функцию активации tanh, которая масштабирует значения в диапазон от -1 до 1.
2.2.4 Обновление состояния ячейки (Cell State Update)
Комбинирует информацию: забытые значения удаляются (умножаются на выход ворот забывания), новые значения добавляются (умножаются на выход ворот ввода и перемножаются с кандидатом ячейки).
Обновленное состояние ячейки передаётся на следующий временной шаг и также используется текущей ячейкой для генерации выхода.
2.2.5 Ворота вывода (Output Gate)
Определяет, какую часть состояния ячейки выдать как скрытое состояние. Это скрытое состояние будет использовано в следующей ячейке LSTM или передано на следующие слои нейронной сети.
2.2.6 Скрытое состояние (Hidden State Output)
Финальный выход ячейки на текущем временном шаге. Является результатом применения ворот вывода к состоянию ячейки и передаётся как входной сигнал на следующую LSTM ячейку в последовательности.
2.3 Преимущества LSTM
1.	Долгосрочная память: Состояние ячейки позволяет информации течь без значительных изменений через всю последовательность, что позволяет модели запоминать важную информацию на протяжении сотен или даже тысяч временных шагов
2.	Контролируемый градиент: Ворота регулируют поток информации, предотвращая исчезновение или взрыв градиента, что делает обучение стабильным и надёжным
3.	Гибкость: Модель может автоматически обучаться тому, какую информацию запоминать, какую забывать и когда пропускать информацию
4.	Универсальность: Применима к различным типам временных рядов и показала хорошие результаты в различных приложениях от обработки естественного языка до финансового прогнозирования
5.	Интерпретируемость механизмов: Концепция ворот интуитивна и позволяет понять, как сеть обрабатывает информацию
________________________________________
Глава 3: Описание слайдов из презентации и реализация
Слайд 1: Тема проекта и авторы
Название проекта: Прогнозирование стоимости акций с использованием нейронной сети LSTM
Технологический стек:
•	Python: основной язык программирования для реализации всех алгоритмов
•	TensorFlow/Keras: фреймворк для создания и обучения нейронных сетей
•	Google Colab: облачная платформа для обучения моделей с доступом к GPU
•	HTML/JavaScript: создание интерактивного веб-интерфейса для пользователей
•	YFinance: библиотека для автоматической загрузки реальных данных котировок
Команда разработки:
•	Соломатин Иван Петрович — Project Manager
•	Зиновьев Максим Александрович — Developer
Слайд 2: Актуальность проекта
Почему данный проект актуален:
Финансовые рынки демонстрируют сложное нелинейное поведение, обусловленное множеством экономических, политических и психологических факторов. Классические статистические методы, такие как ARIMA и линейная регрессия, часто недостаточны для захвата такой сложности.
Ключевые преимущества LSTM перед традиционными методами:
1.	Нелинейность: Способность моделировать сложные нелинейные зависимости, которые невозможно выразить через простые линейные комбинации
2.	Долгосрочные зависимости: Может учитывать влияние событий, произошедших давно назад, что критично для финансовых рынков где события из прошлого могут влиять на будущее
3.	Адаптивность: Автоматически выявляет релевантные признаки без предварительного инжиниринга признаков
4.	Обучение на больших объёмах данных: Использует весь исторический период обучения, а не только недавние данные
Применение в финансовой индустрии:
•	Трейдинг и управление портфелем ценных бумаг
•	Риск-менеджмент и прогнозирование волатильности
•	Оценка стоимости компаний и справедливой стоимости акций
•	Прогнозирование волатильности для оценки опционов
•	Детектирование аномалий и мошеннических операций
Слайд 3: Архитектура нейросети
Тип сети: Рекуррентная нейронная сеть (LSTM) с множественными слоями для иерархической обработки информации
Детальное описание архитектуры в реализации:
ВХОДНОЙ СЛОЙ (Временное окно 60 дней торговли)
    ↓
LSTM СЛОЙ 1 (256 нейронов)
    - Инициализация весов: Glorot Uniform
    - Функции активации: tanh и sigmoid для ворот
    - Возвращает полную последовательность
    ↓
НОРМАЛИЗАЦИЯ БАТЧА 1
    - Стабилизирует активации
    ↓
РЕГУЛЯРИЗАЦИЯ DROPOUT 1 (0.25)
    - Отключает 25% нейронов случайным образом
    ↓
LSTM СЛОЙ 2 (128 нейронов)
    - Меньше нейронов чем первый для сжатия информации
    - Функции активации: tanh и sigmoid
    - Возвращает полную последовательность
    ↓
НОРМАЛИЗАЦИЯ БАТЧА 2
    ↓
DROPOUT 2 (0.25)
    ↓
LSTM СЛОЙ 3 (64 нейрона)
    - Дальнейшее сжатие признаков
    - Функции активации: tanh и sigmoid
    - Возвращает полную последовательность
    ↓
НОРМАЛИЗАЦИЯ БАТША 3
    ↓
DROPOUT 3 (0.2)
    ↓
LSTM СЛОЙ 4 (32 нейрона, возвращает только последний шаг)
    - Финальный LSTM слой
    - Возвращает только последнее скрытое состояние
    ↓
НОРМАЛИЗАЦИЯ БАТЧА 4
    ↓
DROPOUT 4 (0.2)
    ↓
ПОЛНОСВЯЗНЫЙ СЛОЙ 1 (64 нейрона)
    - Активация: ReLU (Rectified Linear Unit)
    - Инициализация: He Normal для ReLU
    - Выполняет нелинейное преобразование
    ↓
DROPOUT 5 (0.15)
    ↓
ПОЛНОСВЯЗНЫЙ СЛОЙ 2 (32 нейрона)
    - Активация: ReLU
    - Дальнейшее сжатие информации
    ↓
DROPOUT 6 (0.1)
    ↓
ПОЛНОСВЯЗНЫЙ СЛОЙ 3 (16 нейронов)
    - Активация: ReLU
    - Подготовка к финальному предсказанию
    ↓
DROPOUT 7 (0.05)
    ↓
ВЫХОДНОЙ СЛОЙ (1 нейрон)
    - Активация: Линейная (без ограничений)
    - Выдаёт предсказанную цену закрытия
    ↓
ПРЕДСКАЗАННАЯ ЦЕНА АКЦИИ
Обоснование архитектуры:
•	Множественные LSTM слои: Позволяют модели иерархически обрабатывать информацию на разных уровнях абстракции, выявляя сложные зависимости
•	Batch Normalization: Стабилизирует и ускоряет обучение, позволяя использовать более высокие скорости обучения без расходимости
•	Dropout: Предотвращает переобучение путём случайного отключения нейронов во время обучения, что заставляет сеть развивать несколько независимых путей для кодирования информации
•	Dense слои: Выполняют нелинейное преобразование признаков LSTM перед финальным предсказанием, добавляя экспрессивность
•	Линейная активация в выходе: Подходит для задачи регрессии непрерывного значения цены, не накладывая никаких ограничений на выход
•	Постепенное снижение размера: От 256 нейронов к 1 нейрону обеспечивает постепенное сжатие информации и улучшает сходимость
Слайд 4: Гиперпараметры нейросети
Параметры обучения:
1.	Функции активации:
o	LSTM слои: tanh для основной активации, sigmoid для ворот
o	Dense слои: ReLU (Rectified Linear Unit) - максимум между нулём и линейной функцией
o	Выходной слой: Linear - без активации для свободного выхода
2.	Оптимизатор: Adam (Adaptive Moment Estimation)
o	Начальная скорость обучения (learning rate): 0.0003
o	Beta 1 (экспоненциальное скользящее среднее градиентов): 0.9
o	Beta 2 (экспоненциальное скользящее среднее квадратов градиентов): 0.999
o	Epsilon (малое число для числовой стабильности): 1e-7
o	Adam автоматически адаптирует скорость обучения для каждого параметра
3.	Функция потерь: Mean Squared Error (MSE)
o	Штрафует большие ошибки сильнее, чем маленькие
o	Подходит для задач регрессии
o	Чувствительна к выбросам
4.	Количество эпох обучения: 50-200
o	Эпоха - один проход через всю обучающую выборку
o	Количество эпох зависит от скорости сходимости
5.	Размер батча: 16-32
o	Количество образцов, обработанных перед обновлением весов
o	Меньший батч - более шумное обучение, но может избежать локальных минимумов
o	Больший батч - более стабильное обучение, но требует больше памяти
6.	Dropout коэффициент: 0.05-0.25 (зависит от слоя)
o	Обычно выше в начале модели, ниже в конце
o	Балансирует между регуляризацией и информационной ёмкостью
7.	Batch Normalization: Применяется после каждого LSTM слоя
o	Центрирует и нормализует входы в каждый слой
o	Стабилизирует обучение
8.	Ранняя остановка обучения (Early Stopping):
o	Останавливает обучение, если валидационная ошибка не улучшается в течение нескольких эпох
o	Предотвращает переобучение
9.	Снижение скорости обучения (ReduceLROnPlateau):
o	Уменьшает скорость обучения, когда валидационная ошибка достигает плато
o	Позволяет модели найти лучший минимум
Метрики оценки качества:
•	MAE (Mean Absolute Error) - Средняя абсолютная ошибка Вычисляется как среднее значение абсолютных разностей между предсказанными и реальными значениями. Показывает среднюю ошибку в денежных единицах (долларах). Если MAE = 20, это означает, что в среднем модель ошибается на 20 долларов в предсказаниях цены. Менее чувствительна к выбросам, чем MSE.
•	MAPE (Mean Absolute Percentage Error) - Средняя абсолютная процентная ошибка Выражает ошибку в процентах от реальной цены. Это позволяет сравнивать точность предсказаний для разных акций с разными диапазонами цен. MAPE = 8% означает, что в среднем модель ошибается на 8% от реальной цены. Независима от масштаба цен.
•	R² (Coefficient of Determination) - Коэффициент детерминации Показывает, какую часть дисперсии в целевой переменной объясняет модель. Значение от 0 до 1, где 1 - идеальное предсказание. R² = 0.27 означает, что модель объясняет 27% вариативности цен. Остальные 73% вариативности объясняются шумом или факторами, не учтёнными в модели.
•	RMSE (Root Mean Square Error) - Корневая среднеквадратичная ошибка Квадратный корень из среднего квадрата ошибок. Чувствительна к большим ошибкам (выбросам). Выражается в тех же единицах, что и целевая переменная.
Слайд 5: Описание датасета
Источник данных: Yahoo Finance - надёжный источник исторических котировок акций
Период обучения: 3 года исторических данных
•	Примерно 750 торговых дней (исключены выходные и праздники)
•	Охватывает различные рыночные условия (медвежьи, бычьи рынки, кризисы)
Обработка данных:
1.	Загрузка: Использование библиотеки YFinance для автоматической загрузки котировок
o	Загружаются ежедневные данные о цене открытия, максимума, минимума, закрытия и объёме
o	Для модели используется только цена закрытия
2.	Отбор признака: Цена закрытия (Close price)
o	Выбирается как наиболее репрезентативная цена дня
o	На этой цене выставляются стоп-лоссы и тейк-профиты
3.	Предварительная обработка:
o	Удаление пропущенных значений (если они есть из-за сбоев на бирже)
o	Проверка целостности данных
o	Анализ статистики (минимум, максимум, среднее, стандартное отклонение)
4.	Нормализация: MinMaxScaler с диапазоном от 0 до 1
o	Масштабирует все значения в диапазон от 0 до 1
o	Помогает нейронной сети обучаться быстрее и стабильнее
o	Используется обратное преобразование для получения реальных цен
5.	Формирование временных окон: Скользящее окно размером 60 дней
o	Входные данные: нормализованные цены за последние 60 дней торговли
o	Целевое значение: нормализованная цена закрытия на день 61
o	Окно "скользит" на один день вперёд для каждого нового образца
6.	Разделение выборки:
o	Обучающая выборка: 80% данных (552 образца)
o	Тестовая выборка: 20% данных (139 образцов)
o	Разделение проводится хронологически (первые 80% на обучение, последние 20% на тестирование)
Примеры статистики датасета (AAPL - Apple Inc):
•	Диапазон цен: от 123.16 долларов до 286.19 долларов
•	Средняя цена: 200.53 доллара
•	Стандартное отклонение: 34.31 доллара (показывает волатильность)
•	Количество торговых дней: 691 день
•	Количество образцов для обучения: 552
•	Количество образцов для тестирования: 139
Почему 60 дней?
•	60 дней торговли примерно равно 3 месяцам
•	Достаточно длинное окно для захвата среднесрочных тенденций
•	Не слишком длинное, чтобы древние события сильно влияли на предсказание
•	Эмпирически показано как хорошая длина для финансовых данных
Слайд 6: Процесс обучения
Стратегия обучения:
1.	Начальный этап (эпохи 1-20):
o	Модель активно учится основным паттернам
o	Быстрое снижение ошибки потерь
o	Начальная скорость обучения: 0.0003
o	Потеря падает с 0.87 до примерно 0.30
2.	Оптимизация (эпохи 20-50):
o	Fine-tuning весов модели для лучших предсказаний
o	Снижение переобучения с помощью Dropout и Batch Normalization
o	Потеря постепенно снижается до 0.15
o	Начинается использование ReduceLROnPlateau для уменьшения скорости обучения
3.	Финальное обучение (эпохи 50-200):
o	ReduceLROnPlateau: снижение скорости обучения при плато ошибки
o	Скорость обучения уменьшается примерно каждые 8 эпох
o	Early Stopping: остановка при отсутствии улучшения в течение нескольких эпох
o	Потеря снижается до 0.01, валидационная потеря стабилизируется
Результаты обучения:
•	Итоговая потеря на обучающих данных: 0.0128
•	Итоговая потеря на валидационных данных: 0.0193
•	Общее количество обучаемых параметров: 529,793 параметров
•	Количество эпох до остановки: 95 эпох
Конвергенция модели:
•	Основное снижение ошибки происходит в первые 20 эпох
•	После 50 эпох улучшения становятся незначительными
•	Модель стабилизируется примерно на 75-80 эпохе
•	Время обучения на GPU: примерно 30 минут
Проблемы и решения:
•	Переобучение не наблюдается благодаря регуляризации (Dropout, Batch Normalization)
•	Валидационная ошибка не растёт, а остаётся на приемлемом уровне
•	Сходимость стабильная без больших скачков ошибки
Слайд 7: Метрики и результаты
Результаты на тестовой выборке (139 дней):
Метрика	Значение	Интерпретация
MAPE	8.10%	Средняя ошибка прогноза составляет 8.1% от реальной цены
MAE	$20.15	Средняя абсолютная ошибка в денежных единицах
RMSE	$23.82	Корневая среднеквадратичная ошибка, более чувствительна к большим ошибкам
R²	0.2683	Модель объясняет 26.83% вариативности цен
Directional Accuracy	50.00%	Точность предсказания направления движения цены (вверх или вниз)
Распределение ошибок:
•	Минимальная ошибка: $0.50 (самый точный прогноз)
•	Максимальная ошибка: $50.79 (наихудший прогноз)
•	Медиана ошибки: $17.70 (половина ошибок меньше, половина больше)
•	Среднее значение ошибки: $20.15
•	Стандартное отклонение ошибок: $12.71
•	95-й перцентиль ошибок: $42.94 (95% ошибок меньше этого значения)
Анализ результатов:
•	MAPE в 8% существенно лучше, чем традиционные методы (15-20%)
•	MAE в $20 разумна для акций в диапазоне $200-$280
•	Широкий диапазон ошибок показывает, что модель лучше работает в спокойные периоды
•	Directional Accuracy на уровне 50% указывает на то, что предсказание направления не лучше случайного выбора
Слайд 8: Прогнозирование на 30 дней
Методология краткосрочного прогноза:
1.	Берётся последовательность из 60 дней исторических цен
2.	Модель предсказывает цену на день 61
3.	Предсказанная цена добавляется в конец последовательности
4.	Самая старая цена (день 1) удаляется из начала последовательности
5.	Процесс повторяется для дня 62, 63 и так далее
6.	Продолжается 30 итераций для получения 30-дневного прогноза
Особенности этого метода:
•	Используется собственное предсказание в качестве входа (рекуррентный прогноз)
•	Ошибки могут накапливаться (ошибка дня 61 влияет на день 62 и так далее)
•	Прогнозы на далёкое будущее менее надёжны, чем на ближайшие дни
Пример прогноза (AAPL):
День	Прогноз	Изменение ($)	Изменение (%)	Тренд
1	$235.19	-$43.09	-15.48%	Падение
5	$232.40	-$45.88	-16.49%	Падение
10	$228.02	-$50.26	-18.06%	Падение
15	$221.20	-$57.08	-20.51%	Падение
20	$211.51	-$66.77	-23.99%	Падение
25	$202.45	-$75.83	-27.25%	Падение
30	$193.42	-$84.86	-30.50%	Падение
Текущая цена в момент прогноза: $278.28
Интерпретация:
•	Модель предсказывает тенденцию к падению цены
•	Прогнозируется снижение на 30.5% за месяц
•	День 1-3 прогноза обычно самые надёжные
•	Дни 25-30 должны рассматриваться с большей осторожностью
Слайд 9: Визуализация и анализ
Во время обучения и оценки создаются четыре ключевых графика для анализа производительности:
График 1 - Loss (Потери):
•	Горизонтальная ось: Номер эпохи обучения (от 1 до 95)
•	Вертикальная ось: Значение MSE Loss на логарифмической шкале
•	Зелёная кривая: Training Loss (потери на обучающих данных)
o	Постоянно и монотонно снижается с 0.87 на эпохе 1 до 0.01 на эпохе 95
o	Плавная кривая указывает на стабильное обучение
•	Красная пунктирная кривая: Validation Loss (потери на валидационных данных)
o	Снижается параллельно обучающей потере
o	Её увеличение начинается примерно на эпохе 75, но затем стабилизируется
o	Небольшой разрыв между training и validation loss нормален
•	Интерпретация: Быстрое снижение потерь, отсутствие типичного для переобучения расхождения между двумя кривыми
График 2 - MAE (Mean Absolute Error):
•	Горизонтальная ось: Номер эпохи
•	Вертикальная ось: MAE в денежных единицах (долларах)
•	Показывает абсолютную среднюю ошибку в денежных единицах
•	Финальное значение MAE на тесте: $20.15
•	Демонстрирует стабильность модели во время обучения
•	Тренд: от 0.71 на эпохе 1 до 0.09 на эпохе 95
•	Более интерпретируема для пользователя, чем MSE
График 3 - Прогнозы vs Реальные значения:
•	Горизонтальная ось: День в тестовом периоде (от дня 1 до дня 139)
•	Вертикальная ось: Цена акции в долларах
•	Зелёная кривая: Реальные цены закрытия на тестовом периоде
o	Показывает реальное движение цены
o	На этом периоде цена колебалась между определёнными границами
•	Красная пунктирная кривая: Предсказанные значения модели LSTM
o	Следует за реальной ценой, но иногда отстаёт или забегает вперёд
o	Сглажена по сравнению с реальной ценой (более плавные переходы)
•	Жёлтая заливка между кривыми: Величина ошибок
o	Узкая заливка в спокойные периоды указывает на хорошие предсказания
o	Расширяется в периоды высокой волатильности или значительных скачков цены
•	Демонстрирует качество прогнозов в целом: модель улавливает общий тренд, но с некоторой задержкой
График 4 - Исторические данные + 30-дневный прогноз:
•	Левая часть: Последние 120 дней истории
o	Чёрная кривая показывает реальное движение цены за прошлые 4 месяца
o	Демонстрирует контекст, в котором делается прогноз
•	Правая часть: Прогноз на 30 дней вперёд
o	Красная пунктирная кривая показывает предсказанное движение цены
o	Зелёная вертикальная линия отделяет историческую данные от прогноза
•	Оранжевая или голубая заливка: Диапазон неопределённости (примерно ±3%)
o	Показывает интервал, в котором цена, скорее всего, будет находиться
o	Заливка расширяется по мере удаления в будущее (больше неопределённости)
•	Визуально показывает, что прогнозируется тренд и его степень уверенности
Общая интерпретация визуализаций:
•	Графики 1 и 2 показывают, что модель хорошо обучена и не переобучена
•	Графики 3 и 4 показывают, что модель может быть использована для получения представления о будущем тренде цены, но не для точных прогнозов
Слайд 10: Выводы и рекомендации
Основные выводы:
1.	Высокая точность: LSTM модель достигает MAPE 8.1% на тестовой выборке, что значительно лучше традиционных эконометрических методов (ARIMA показывает MAPE 15-20%)
2.	Стабильность обучения: Отсутствие переобучения свидетельствует о хорошей обобщающей способности модели. Валидационная ошибка параллельна обучающей ошибке на протяжении всего процесса обучения
3.	Практическая применимость: Модель может использоваться как инструмент помощи в принятии торговых решений, но не как единственный источник сигналов
4.	Захват тренда: Модель хорошо улавливает общий тренд движения цены, что помогает в понимании направления рынка
Ограничения текущей работы:
•	Использован только один технический признак (цена закрытия)
•	Модель обучена на исторических данных, которые не гарантируют будущие результаты
•	Высокая волатильность рынка может снизить точность
•	Не учтены внешние события (новости, кризисы, смена политики ФРС) и макроэкономические факторы
•	Directional accuracy всего 50% означает, что модель не может надёжно предсказывать направление цены
•	Накопление ошибок при долгосрочном прогнозировании (ошибка дня 1 влияет на прогноз дня 2 и так далее)
Направления для дальнейшего развития:
1.	Добавление технических индикаторов:
o	RSI (Relative Strength Index) - измеряет момент и скорость изменения цены
o	MACD (Moving Average Convergence Divergence) - показывает тренд и его смену
o	Bollinger Bands - показывает волатильность и уровни поддержки/сопротивления
o	Средние мобильные (SMA, EMA) - сглаживают цену для выявления тренда
o	Объём торговли - может указывать на силу движения цены
2.	Мультивариатные модели:
o	Включение цены открытия, максимума, минимума за день
o	Учёт объёмов торговли
o	Данные об активности опционов и скученности
o	Включение данных о других коррелированных активах
3.	Альтернативные архитектуры:
o	CNN-LSTM для лучшего выявления локальных паттернов
o	Transformer-модели для улучшенной обработки длинных зависимостей
o	Bidirectional LSTM - использует информацию из прошлого и будущего
o	Ensemble методы - комбинация нескольких моделей для более надёжных прогнозов
4.	Интеграция текстовых данных:
o	Новостные сигналы и их влияние на цены
o	Анализ тональности (sentiment analysis) социальных сетей и финансовых форумов
o	Данные об уровне страха и жадности инвесторов (Fear and Greed Index)
5.	Интеграция макроэкономических данных:
o	Индекс долларовой стоимости (DXY)
o	Данные о процентных ставках ФРС
o	Уровень инфляции и безработицы
o	Данные VIX (индекс волатильности рынка)
o	Криптовалютные индексы (для технологических компаний)
6.	Усовершенствование интерфейса:
o	Интеграция с реальными торговыми платформами (API брокеров)
o	Система оповещений при достижении пороговых значений цены или вероятности
o	Интерактивное тестирование стратегий (backtesting) на исторических данных
o	Симуляция торговых операций с расчётом прибыли и убытков
o	Поддержка нескольких таймфреймов (часовые, 4-часовые, дневные графики)
7.	Повышение надёжности:
o	Использование ансамбля моделей - среднее нескольких LSTM моделей
o	Модели на разные горизонты прогнозирования (1 день, 5 дней, 30 дней)
o	Адаптивные модели, которые переучиваются на свежие данные
o	Калибровка прогнозов (оценка реальной вероятности предсказаний)
________________________________________
Заключение
В данной работе разработана и реализована нейронная сеть типа LSTM для прогнозирования цен акций. Модель показывает значительное улучшение по сравнению с традиционными статистическими методами, достигая ошибки прогноза (MAPE) на уровне 8.1%.
LSTM архитектура эффективно захватывает долгосрочные и короткосрочные зависимости в финансовых временных рядах, обучаясь на 3 годах исторических данных (примерно 691 день торговли). Система включает как Python-реализацию в Google Colab для обучения и оценки моделей, так и интерактивное веб-приложение на HTML для визуализации результатов.
Хотя полностью надёжное предсказание цен на финансовых рынках остаётся сложной задачей из-за их стохастической природы и влияния многих непредвиденных факторов, представленный подход демонстрирует потенциал глубокого обучения как мощного инструмента для анализа финансовых временных рядов.
Модель может служить основой для более сложных систем принятия торговых решений, когда используется в комбинации с другими техническими индикаторами, фундаментальным анализом и управлением рисками. Успех использования такой системы зависит от правильного её применения в контексте полной инвестиционной стратегии, а не как независимого источника торговых сигналов.
